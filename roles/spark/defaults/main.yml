---
spark_version: "2.1.1-bin-without-hadoop"
spark_mirror: "http://d3kbcqa49mib13.cloudfront.net"
spark_dir: "/data/local/spark"
spark_src_dir: "/usr/local/src"
spark_conf_dir: "/etc/spark"
spark_usr_parent_dir: "/usr/lib"  #this is the folder where the spark archive will be extracted
spark_usr_dir: "/usr/lib/spark"   #this is the symlink to the extracted/installed spark
spark_lib_dir: "/var/lib/spark"
spark_log_dir: "/var/log/spark"
spark_run_dir: "/run/spark"
spark_user: "spark"           # the name of the (OS)user created for spark
spark_user_groups: [ users ]         # Optional list of (OS)groups the new spark user should belong to
spark_user_shell: "/bin/false"    # the spark user's default shell
spark_worker_cores: 2
spark_executor_memory: 2
spark_worker_memory: 2
spark_driver_memory: 1
spark_daemon_memory: 3
spark_driver_maxResultSize: 1
spark_dynamicAllocation_initialExecutors: 1
spark_dynamicAllocation_maxExecutors: 2
spark_broadcast_blockSize: 20

joda_time_version: "2.9.4"
aws_java_sdk_s3_version: "1.10.6"
hadoop_aws_version: "2.8.0"
spark_classpath_extras: [ "joda-time-{{ joda_time_version }}.jar", "aws-java-sdk-s3-{{ aws_java_sdk_s3_version }}.jar", "hadoop-aws-{{ hadoop_aws_version }}.jar" ]
spark_env_extras: {}
spark_defaults_extras: { spark.kryoserializer.buffer.max 256m, spark.shuffle.service.enabled true, spark.shuffle.service.port 7338 }
