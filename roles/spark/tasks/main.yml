---
#Create environment for Spark
- name: Add ppa:webupd8team/java
  apt_repository: repo=ppa:webupd8team/java
- name: R ppa key
  apt_key: id="{{ cran_ppa_key }}" state=present keyserver=keyserver.ubuntu.com
- name: R ppa
  apt_repository: repo='deb {{ cran_url }}/bin/linux/ubuntu xenial/' state=present
- name: Update apt cache
  apt: update_cache=yes
- name: accept Oracle license
  debconf: name='oracle-java8-installer' question='shared/accepted-oracle-license-v1-1' vtype='select' value='true'
- name: Oracle JDK8
  apt: name={{ item }}
  with_items:
    - oracle-java8-installer
    - oracle-java8-set-default
    - oracle-java8-unlimited-jce-policy
- name: numpy
  apt: name=python-numpy
- name: libnet
  apt: name=libnetlib-java
- name: R
  apt: name=r-recommended
- name: R javareconf
  command: R CMD javareconf
- name: RJava
  apt: name=r-cran-rjava

#Setting up Spark
- name: Ensure Spark directory exists
  file: path="{{ spark_dir }}" state=directory owner=ubuntu group=users mode=g+rw

- name: Ensure Spark events directory exists
  file: path=/tmp/spark-events state=directory owner=ubuntu group=users mode=g+rw

- name: Create service account for Spark
  user: name={{ spark_user }}
        system=yes
        home={{ spark_lib_dir }}
        shell={{ spark_user_shell }}
        state=present
        groups="{{ spark_user_groups | join(',') }}"
  tags: ["spark-user"]

- name: Ensure Spark configuration directory exists
  file: path="{{ spark_conf_dir }}"
        state=directory
  tags: ["config"]

- name: Ensure Spark log and run directories exist
  file: path="{{ item }}"
        owner={{ spark_user }}
        group={{ spark_user }}
        mode=0755
        state=directory
  with_items:
    - "{{ spark_log_dir }}"
    - "{{ spark_run_dir }}"

- name: Download Spark distribution
  get_url: url="{{ spark_mirror }}/spark-{{ spark_version }}.tgz"
           dest="{{ spark_src_dir }}/spark-{{ spark_version }}.tgz"

- name: Extract Spark distribution
  unarchive: src="{{ spark_src_dir }}/spark-{{ spark_version }}.tgz"
             dest="{{ spark_usr_parent_dir }}"
             copy=no
             creates="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}"

- name: Setup Spark distribution symlinks
  file: src="{{ item.src }}"
        dest="{{ item.dest }}"
        state=link
  with_items:
    - { src: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}", dest: "{{ spark_usr_dir }}" }
    - { src: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf", dest: "{{ spark_conf_dir }}/conf" }
  tags: ["symlinks"]

- name: Create shims for Spark binaries
  template: src=spark-shim.j2
            dest="/usr/bin/{{ item }}"
            mode=0755
  with_items:
    - spark-class
    - spark-shell
    - spark-sql
    - spark-submit
  tags: ["shims"]

- name: install maven
  apt: name=maven

#Downland necessary jars using maven
- name: Maven hadoop-aws
  maven_artifact:
    group_id: org.apache.hadoop
    artifact_id: hadoop-aws
    version: 2.7.3
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/hadoop-aws-2.7.3.jar"

- name: Maven aws-java-sdk
  maven_artifact:
    group_id: com.amazonaws
    artifact_id: aws-java-sdk
    version: 1.7.4
    repository_url: http://central.maven.org/maven2/
    dest: "{{ spark_usr_dir }}/jars/aws-java-sdk-1.7.4.jar"

- name: Configure Spark environment
  template: src=spark-env.sh.j2
            dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-env.sh"
  tags: ["config"]

- name: Configure Spark defaults config file
  template: src=spark-defaults.conf.j2
            dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-defaults.conf"
  tags: ["config"]

- name: Configure Hive defaults site file
  template: src=hive-site.xml.j2
            dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/hive-site.xml"
  tags: ["config"]

